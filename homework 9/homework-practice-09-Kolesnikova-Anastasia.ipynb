{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Машинное обучение, ФКН ВШЭ\n",
    "\n",
    "# Практическое задание 9. EM-алгоритм\n",
    "\n",
    "## Общая информация\n",
    "\n",
    "Дата выдачи: 28.02.2024\n",
    "\n",
    "Мягкий дедлайн: 19.03.2024 23:59 MSK\n",
    "\n",
    "Жёсткий дедлайн: 25.03.2024 23:59 MSK\n",
    "\n",
    "## Оценивание и штрафы\n",
    "Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимальная оценка за работу (без учёта бонусов) — 15 баллов.\n",
    "\n",
    "Сдавать задание после указанного жёсткого срока сдачи нельзя. При выставлении неполного балла за задание в связи с наличием ошибок на усмотрение проверяющего предусмотрена возможность исправить работу на указанных в ответном письме условиях.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов (подробнее о плагиате см. на странице курса). Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке.\n",
    "\n",
    "## Формат сдачи\n",
    "Задания сдаются через систему anytask. Посылка должна содержать:\n",
    "* Ноутбук homework-practice-09-em-Username.ipynb\n",
    "* Модули preprocessing.py, metrics.py, models.py, содержащие написанный вами код\n",
    "* Ссылки на посылки в Яндекс.Контест для всех функций и классов, которые вы реализовали\n",
    "\n",
    "Ссылка на Яндекс.Контест: https://contest.yandex.ru/contest/60281\n",
    "\n",
    "Username — ваша фамилия и имя на латинице именно в таком порядке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Generative model of Labels, Abilities, and Difficulties (GLAD)\n",
    "\n",
    "В [семинаре 16](https://github.com/esokolov/ml-course-hse/blob/master/2022-spring/seminars/sem16-em.pdf) мы рассмотрели задачу восстановления истинной разметки по меткам от экспертов (которым мы не можем доверять в полной мере, более того, их предсказания могут расходиться).\n",
    "\n",
    "Рассмотрим следующую вероятностную модель:\n",
    "\n",
    "$$ p(L, Z | \\alpha, \\beta) = \\prod_{i=1}^{n} \\prod_{j=1}^m \\sigma(\\alpha_j\\beta_i)^{[l_{ij}=z_i]}\\sigma(-\\alpha_j\\beta_i)^{1-[l_{ij}=z_i]} p(z_j)$$\n",
    "\n",
    "где $l_{ij} -$ ответ $j$-го эксперта на задачу $i$, $z_j -$ истинная разметка, $\\alpha_i, \\beta_j-$ уровень экспертизы и сложность задачи соответственно. Для более подробного описания модели можно прочитать материалы семинара, а также [оригинальную статью](http://papers.nips.cc/paper/3644-whose-vote-should-count-more-optimal-integration-of-labels-from-labelers-of-unknown-expertise.pdf). Априорное распределение положим равномерным: $p(z_i) = 0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "seed = 0xDEADF00D\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "L = np.load('L.npy')\n",
    "n, m = L.shape\n",
    "n_problems, n_experts = L.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Задание 1. (2 балла)** Реализуйте EM-алгоритм для заданной выше модели. Вы можете воспользоваться предложенными шаблонами или написать свои. \n",
    "\n",
    "Обратите внимание, что правдоподобие моделирует не вероятность метки $l_{ij}$ принять значение 1 или 0, а вероятность того, что она равна скрытой переменной $z_i$, т.е. $p(l_{ij} = z_j|z_j, \\alpha_j, \\beta_i) \\neq p(l_{ij} = 1|\\alpha_j, \\beta_i) $. При этом заранее неизвестно, какая из скрытых переменных соответствует метке 1. Не забывайте, что параметры $\\beta_i$ должны быть неотрицательными; для этого оптимизируйте $\\log \\beta$. На M-шаге можете использовать как один шаг градиентного спуска, так и несколько: разумные результаты у вас должны получаться вне зависимости от числа итераций.\n",
    "\n",
    "Также при работе с вероятностями не забывайте о точности:\n",
    "1. Используйте логарифмы вероятностей.\n",
    "2. $\\log \\sigma(a)$ лучше преобразовать в $\\log \\sigma(a) = -\\log(1 + \\exp(-a)) = -\\mathrm{softplus}(-a) $\n",
    "3. Ещё полезные функции: `scipy.special.expit`, `scipy.special.logsumexp`, `np.log1p`\n",
    "\n",
    "Для отладки может быть полезно проверить градиент с помощью `scipy.optimize.check_grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def softplus(x):\n",
    "    '''stable version of log(1 + exp(x))'''\n",
    "    c = (x > 20) * 1.\n",
    "    return np.log1p(np.exp(x * (1-c)) * (1-c)) + x * c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код в матричном виде."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "log_prior = np.log(np.full(n_problems, 0.5))\n",
    "# получается вектор, или массив размерности (1, n_problems), или же (n_problems,)\n",
    "\n",
    "\n",
    "def log_likelihood(alpha, beta, L, z):\n",
    "    \"\"\" p(l=z|z, \\alpha, \\beta), логарифм вероятности того, что метки l равны заданным меткам z при условии параметров модели\n",
    "    Args:\n",
    "        alpha: ndarray of shape (n_experts).\n",
    "        beta: ndarray of shape (n_problems).\n",
    "        L: ndarray of shape (n_problems, n_experts).\n",
    "        z: ndarray of shape (n_problems).\n",
    "    \"\"\"\n",
    "    log_likelihood = np.zeros((n_problems, n_experts))\n",
    "\n",
    "    # alpha_beta = beta[:, np.newaxis] * alpha[np.newaxis, :]\n",
    "    alpha_beta = np.outer(beta, alpha)\n",
    "    # размерность массива alpha_beta: (n_problems, n_experts)\n",
    "\n",
    "    # L == z: возаращает массив (матрицу) булевых значений, показывающих, равен ли каждый элемент вектора\n",
    "    condition = (L == z)\n",
    "    log_likelihood[condition] = -softplus(-alpha_beta[condition])\n",
    "    log_likelihood[~condition] = -softplus(alpha_beta[~condition])\n",
    "\n",
    "    log_likelihood = np.einsum('ij->i', log_likelihood)\n",
    "    # теперь log_likelihood - вектор, или массив размерности (1, n_problems), или же (n_problems,)\n",
    "    return log_likelihood\n",
    "\n",
    "\n",
    "def posterior(alpha, beta, L):\n",
    "    \"\"\" Posterior over true labels z p(z|l, \\alpha, \\beta)\n",
    "    Args:\n",
    "        alpha: ndarray of shape (n_experts).\n",
    "        beta: ndarray of shape (n_problems).\n",
    "        L: ndarray of shape (n_problems, n_experts).\n",
    "    \"\"\"\n",
    "    \n",
    "    log_likelihood_0 = log_likelihood(alpha, beta, L, z=0)\n",
    "    log_likelihood_1 = log_likelihood(alpha, beta, L, z=1)\n",
    "    # а дальше на каком-то шаге нужно применить np.vstack, чтобы получить размерность (1, n_problems) у апостериорного распределения\n",
    "\n",
    "    q_0 = log_prior + log_likelihood_0\n",
    "    q_1 = log_prior + log_likelihood_1\n",
    "    # я хз, как работает софтмакс, но вроде можно и к многомерному массиву применять, типа независимо построчно сделает. если получися какая-то ересь, глянуть\n",
    "\n",
    "    q = softmax(np.vstack([q_0, q_1]), axis=0)\n",
    "    # у q размерность (2, n_problems)\n",
    "\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "\n",
    "def AB(alpha, beta, L, z):\n",
    "    \"\"\" p(l=z|z, \\alpha, \\beta)\n",
    "    Args:\n",
    "        alpha: ndarray of shape (n_experts).\n",
    "        beta: ndarray of shape (n_problems).\n",
    "        L: ndarray of shape (n_problems, n_experts).\n",
    "        z: ndarray of shape (n_problems).\n",
    "    \"\"\"\n",
    "    AB_true = np.zeros((n_problems, n_experts))\n",
    "    AB_false = np.zeros((n_problems, n_experts))\n",
    "\n",
    "    alpha_beta = np.outer(beta, alpha)\n",
    "    # размерность массива alpha_beta: (n_problems, n_experts)\n",
    "\n",
    "    # L == z: возаращает массив (матрицу) булевых значений, показывающих, равен ли каждый элемент вектора\n",
    "    condition = (L == z)\n",
    "    AB_true[condition] = expit(alpha_beta[condition])\n",
    "    AB_false[~condition] = expit(-alpha_beta[~condition])\n",
    "\n",
    "    AB_matrix = AB_true - AB_false\n",
    "    # теперь AB - матрица размерности (n_problems, n_experts)\n",
    "    return AB_matrix\n",
    "\n",
    "\n",
    "def alpha_grad_lb(alpha, beta, L, q):\n",
    "    \"\"\" Gradient of lower bound wrt alpha\n",
    "    Args:\n",
    "        alpha: ndarray of shape (n_experts).\n",
    "        beta: ndarray of shape (n_problems).\n",
    "        L: ndarray of shape (n_problems, n_experts).\n",
    "        q: ndarray of shape (2, n_problems).\n",
    "    \"\"\"\n",
    "\n",
    "    AB_matrix_0 = AB(alpha, beta, L, z=0)\n",
    "    AB_matrix_1 = AB(alpha, beta, L, z=1)\n",
    "    AB_matrix = np.stack([AB_matrix_0, AB_matrix_1])\n",
    "\n",
    "    beta_q = beta * q\n",
    "\n",
    "    grad_alpha = np.tensordot(beta_q, AB_matrix) # 2x2000 @ 2x2000x20 > 1x20\n",
    "\n",
    "    return grad_alpha\n",
    "\n",
    "\n",
    "def logbeta_grad_lb(alpha, beta, L, q):\n",
    "    \"\"\" Gradient of lower bound wrt alpha\n",
    "    Args:\n",
    "        alpha: ndarray of shape (n_experts).\n",
    "        beta: ndarray of shape (n_problems).\n",
    "        L: ndarray of shape (n_problems, n_experts).\n",
    "        q: ndarray of shape (2, n_problems).\n",
    "    \"\"\"\n",
    "    \n",
    "    alpha_q = np.tensordot(q, alpha, axes=0)\n",
    "\n",
    "    AB_matrix_0 = AB(alpha, beta, L, z=0)\n",
    "    AB_matrix_1 = AB(alpha, beta, L, z=1)\n",
    "    AB_matrix = np.stack([AB_matrix_0, AB_matrix_1])\n",
    "\n",
    "    sum1 = np.sum([alpha_q, AB_matrix], axis=3)\n",
    "    grad_beta = np.sum(sum1, axis=(0, 1)) # размерность (2000,)\n",
    "\n",
    "    return grad_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_bound(alpha, beta, L, q):\n",
    "    \"\"\" Lower bound\n",
    "    Args:\n",
    "        alpha: ndarray of shape (n_experts).\n",
    "        beta: ndarray of shape (n_problems).\n",
    "        L: ndarray of shape (n_problems, n_experts).\n",
    "        q: ndarray of shape (2, n_problems).\n",
    "    \"\"\"\n",
    "    log_likelihood_0 = log_likelihood(alpha, beta, L, z=0)\n",
    "    log_likelihood_1 = log_likelihood(alpha, beta, L, z=1)\n",
    "\n",
    "    lower_bound_0 = q[0] * log_likelihood_0\n",
    "    lower_bound_1 = q[1] * log_likelihood_1\n",
    "\n",
    "    lower_bound = np.sum(lower_bound_0) + np.sum(lower_bound_1)\n",
    "\n",
    "    return lower_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def em(L, n_steps=1000, lr=1e-3):\n",
    "    # initialize parameters\n",
    "    alpha, logbeta = np.random.randn(m), np.random.randn(n)\n",
    "    q = np.ones((2, len(logbeta))) * 0.5\n",
    "\n",
    "    lower_bound_list = []\n",
    "\n",
    "    for step in range(n_steps):\n",
    "        # E-step\n",
    "        q = posterior(alpha, np.exp(logbeta), L)\n",
    "\n",
    "        # M-step\n",
    "        grad_alpha = alpha_grad_lb(alpha, np.exp(logbeta), L, q)\n",
    "        grad_logbeta = logbeta_grad_lb(alpha, np.exp(logbeta), L, q)\n",
    "        logbeta += lr * grad_logbeta\n",
    "        alpha += lr * grad_alpha\n",
    "\n",
    "        # вычисляем нижнюю оценку на логарифм правдоподобия модели\n",
    "        lb = lower_bound(alpha, np.exp(logbeta), L, q)\n",
    "        lower_bound_list.append(lb)\n",
    "        # print(f'Step {step}: Lower bound = {lb}')\n",
    "\n",
    "    return alpha, np.exp(logbeta), q, lower_bound_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Lower bound = -62976.90121521668\n",
      "Step 1: Lower bound = -83392.83947674448\n",
      "Step 2: Lower bound = -104405.86284215283\n",
      "Step 3: Lower bound = -122802.15819999452\n",
      "Step 4: Lower bound = -136821.40231534274\n",
      "Step 5: Lower bound = -145413.0684979214\n",
      "Step 6: Lower bound = -148509.48627968697\n",
      "Step 7: Lower bound = -146747.63966602448\n",
      "Step 8: Lower bound = -141045.8553787131\n",
      "Step 9: Lower bound = -132445.8997802967\n",
      "Step 10: Lower bound = -121983.09237891364\n",
      "Step 11: Lower bound = -110575.65985522083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 12: Lower bound = -98963.0332935313\n",
      "Step 13: Lower bound = -87704.58039549692\n",
      "Step 14: Lower bound = -77189.26090267187\n",
      "Step 15: Lower bound = -67659.28731993155\n",
      "Step 16: Lower bound = -59239.37846003799\n",
      "Step 17: Lower bound = -51965.96321766901\n",
      "Step 18: Lower bound = -45813.08976962535\n",
      "Step 19: Lower bound = -40713.59089553807\n",
      "Step 20: Lower bound = -36575.280417459086\n",
      "Step 21: Lower bound = -33292.69905756875\n",
      "Step 22: Lower bound = -30755.31641826748\n",
      "Step 23: Lower bound = -28853.19108197545\n",
      "Step 24: Lower bound = -27480.939507847477\n",
      "Step 25: Lower bound = -26540.572407804284\n",
      "Step 26: Lower bound = -25943.47713073509\n",
      "Step 27: Lower bound = -25611.661310296622\n",
      "Step 28: Lower bound = -25478.322276185274\n",
      "Step 29: Lower bound = -25487.80357045895\n",
      "Step 30: Lower bound = -25595.009359897114\n",
      "Step 31: Lower bound = -25764.380435343355\n",
      "Step 32: Lower bound = -25968.59552041629\n",
      "Step 33: Lower bound = -26187.207333053062\n",
      "Step 34: Lower bound = -26405.39499212189\n",
      "Step 35: Lower bound = -26612.90995695366\n",
      "Step 36: Lower bound = -26803.188034035717\n",
      "Step 37: Lower bound = -26972.5693236952\n",
      "Step 38: Lower bound = -27119.60221262358\n",
      "Step 39: Lower bound = -27244.4417752213\n",
      "Step 40: Lower bound = -27348.351189164532\n",
      "Step 41: Lower bound = -27433.29593780691\n",
      "Step 42: Lower bound = -27501.614256606386\n",
      "Step 43: Lower bound = -27555.75636864586\n",
      "Step 44: Lower bound = -27598.09370104816\n",
      "Step 45: Lower bound = -27630.797025063228\n",
      "Step 46: Lower bound = -27655.77349375416\n",
      "Step 47: Lower bound = -27674.646553882107\n",
      "Step 48: Lower bound = -27688.763239916447\n",
      "Step 49: Lower bound = -27699.21754918087\n",
      "Step 50: Lower bound = -27706.882590101868\n",
      "Step 51: Lower bound = -27712.446015002428\n",
      "Step 52: Lower bound = -27716.443891601808\n",
      "Step 53: Lower bound = -27719.289844267678\n",
      "Step 54: Lower bound = -27721.298924062125\n",
      "Step 55: Lower bound = -27722.707336770658\n",
      "Step 56: Lower bound = -27723.689192356196\n",
      "Step 57: Lower bound = -27724.370780034435\n",
      "Step 58: Lower bound = -27724.84244326678\n",
      "Step 59: Lower bound = -27725.16809745505\n",
      "Step 60: Lower bound = -27725.39257920727\n",
      "Step 61: Lower bound = -27725.547145240023\n",
      "Step 62: Lower bound = -27725.653487393083\n",
      "Step 63: Lower bound = -27725.72661156323\n",
      "Step 64: Lower bound = -27725.776875263888\n",
      "Step 65: Lower bound = -27725.811416448876\n",
      "Step 66: Lower bound = -27725.835149010632\n",
      "Step 67: Lower bound = -27725.851453263385\n",
      "Step 68: Lower bound = -27725.862653383578\n",
      "Step 69: Lower bound = -27725.87034683655\n",
      "Step 70: Lower bound = -27725.87563134335\n",
      "Step 71: Lower bound = -27725.87926109889\n",
      "Step 72: Lower bound = -27725.881754221115\n",
      "Step 73: Lower bound = -27725.883466621868\n",
      "Step 74: Lower bound = -27725.88464277665\n",
      "Step 75: Lower bound = -27725.885450610083\n",
      "Step 76: Lower bound = -27725.886005463326\n",
      "Step 77: Lower bound = -27725.886386558854\n",
      "Step 78: Lower bound = -27725.886648310414\n",
      "Step 79: Lower bound = -27725.886828091738\n",
      "Step 80: Lower bound = -27725.886951572655\n",
      "Step 81: Lower bound = -27725.887036384247\n",
      "Step 82: Lower bound = -27725.887094636215\n",
      "Step 83: Lower bound = -27725.887134645996\n",
      "Step 84: Lower bound = -27725.887162126313\n",
      "Step 85: Lower bound = -27725.8871810009\n",
      "Step 86: Lower bound = -27725.887193964732\n",
      "Step 87: Lower bound = -27725.887202868813\n",
      "Step 88: Lower bound = -27725.887208984495\n",
      "Step 89: Lower bound = -27725.887213185\n",
      "Step 90: Lower bound = -27725.887216070078\n",
      "Step 91: Lower bound = -27725.887218051663\n",
      "Step 92: Lower bound = -27725.8872194127\n",
      "Step 93: Lower bound = -27725.88722034751\n",
      "Step 94: Lower bound = -27725.887220989585\n",
      "Step 95: Lower bound = -27725.88722143058\n",
      "Step 96: Lower bound = -27725.88722173348\n",
      "Step 97: Lower bound = -27725.88722194152\n",
      "Step 98: Lower bound = -27725.887222084413\n",
      "Step 99: Lower bound = -27725.887222182555\n",
      "Step 100: Lower bound = -27725.887222249963\n",
      "Step 101: Lower bound = -27725.887222296264\n",
      "Step 102: Lower bound = -27725.887222328067\n",
      "Step 103: Lower bound = -27725.887222349906\n",
      "Step 104: Lower bound = -27725.88722236491\n",
      "Step 105: Lower bound = -27725.88722237521\n",
      "Step 106: Lower bound = -27725.88722238229\n",
      "Step 107: Lower bound = -27725.88722238715\n",
      "Step 108: Lower bound = -27725.887222390487\n",
      "Step 109: Lower bound = -27725.887222392783\n",
      "Step 110: Lower bound = -27725.887222394354\n",
      "Step 111: Lower bound = -27725.88722239544\n",
      "Step 112: Lower bound = -27725.88722239618\n",
      "Step 113: Lower bound = -27725.887222396694\n",
      "Step 114: Lower bound = -27725.887222397047\n",
      "Step 115: Lower bound = -27725.887222397287\n",
      "Step 116: Lower bound = -27725.887222397447\n",
      "Step 117: Lower bound = -27725.887222397563\n",
      "Step 118: Lower bound = -27725.887222397643\n",
      "Step 119: Lower bound = -27725.887222397694\n",
      "Step 120: Lower bound = -27725.88722239773\n",
      "Step 121: Lower bound = -27725.887222397756\n",
      "Step 122: Lower bound = -27725.887222397774\n",
      "Step 123: Lower bound = -27725.887222397785\n",
      "Step 124: Lower bound = -27725.887222397796\n",
      "Step 125: Lower bound = -27725.8872223978\n",
      "Step 126: Lower bound = -27725.887222397803\n",
      "Step 127: Lower bound = -27725.887222397807\n",
      "Step 128: Lower bound = -27725.887222397807\n",
      "Step 129: Lower bound = -27725.887222397807\n",
      "Step 130: Lower bound = -27725.88722239781\n",
      "Step 131: Lower bound = -27725.88722239781\n",
      "Step 132: Lower bound = -27725.88722239781\n",
      "Step 133: Lower bound = -27725.88722239781\n",
      "Step 134: Lower bound = -27725.88722239781\n",
      "Step 135: Lower bound = -27725.88722239781\n",
      "Step 136: Lower bound = -27725.88722239781\n",
      "Step 137: Lower bound = -27725.88722239781\n",
      "Step 138: Lower bound = -27725.88722239781\n",
      "Step 139: Lower bound = -27725.88722239781\n",
      "Step 140: Lower bound = -27725.88722239781\n",
      "Step 141: Lower bound = -27725.88722239781\n",
      "Step 142: Lower bound = -27725.887222397814\n",
      "Step 143: Lower bound = -27725.88722239781\n",
      "Step 144: Lower bound = -27725.88722239781\n",
      "Step 145: Lower bound = -27725.88722239781\n",
      "Step 146: Lower bound = -27725.88722239781\n",
      "Step 147: Lower bound = -27725.88722239781\n",
      "Step 148: Lower bound = -27725.88722239781\n",
      "Step 149: Lower bound = -27725.88722239781\n",
      "Step 150: Lower bound = -27725.88722239781\n",
      "Step 151: Lower bound = -27725.887222397814\n",
      "Step 152: Lower bound = -27725.88722239781\n",
      "Step 153: Lower bound = -27725.88722239781\n",
      "Step 154: Lower bound = -27725.88722239781\n",
      "Step 155: Lower bound = -27725.88722239781\n",
      "Step 156: Lower bound = -27725.88722239781\n",
      "Step 157: Lower bound = -27725.88722239781\n",
      "Step 158: Lower bound = -27725.88722239781\n",
      "Step 159: Lower bound = -27725.88722239781\n",
      "Step 160: Lower bound = -27725.88722239781\n",
      "Step 161: Lower bound = -27725.88722239781\n",
      "Step 162: Lower bound = -27725.88722239781\n",
      "Step 163: Lower bound = -27725.88722239781\n",
      "Step 164: Lower bound = -27725.88722239781\n",
      "Step 165: Lower bound = -27725.88722239781\n",
      "Step 166: Lower bound = -27725.887222397814\n",
      "Step 167: Lower bound = -27725.88722239781\n",
      "Step 168: Lower bound = -27725.88722239781\n",
      "Step 169: Lower bound = -27725.88722239781\n",
      "Step 170: Lower bound = -27725.88722239781\n",
      "Step 171: Lower bound = -27725.887222397814\n",
      "Step 172: Lower bound = -27725.88722239781\n",
      "Step 173: Lower bound = -27725.88722239781\n",
      "Step 174: Lower bound = -27725.88722239781\n",
      "Step 175: Lower bound = -27725.887222397814\n",
      "Step 176: Lower bound = -27725.88722239781\n",
      "Step 177: Lower bound = -27725.88722239781\n",
      "Step 178: Lower bound = -27725.88722239781\n",
      "Step 179: Lower bound = -27725.88722239781\n",
      "Step 180: Lower bound = -27725.88722239781\n",
      "Step 181: Lower bound = -27725.88722239781\n",
      "Step 182: Lower bound = -27725.88722239781\n",
      "Step 183: Lower bound = -27725.88722239781\n",
      "Step 184: Lower bound = -27725.88722239781\n",
      "Step 185: Lower bound = -27725.887222397814\n",
      "Step 186: Lower bound = -27725.88722239781\n",
      "Step 187: Lower bound = -27725.887222397814\n",
      "Step 188: Lower bound = -27725.88722239781\n",
      "Step 189: Lower bound = -27725.88722239781\n",
      "Step 190: Lower bound = -27725.88722239781\n",
      "Step 191: Lower bound = -27725.88722239781\n",
      "Step 192: Lower bound = -27725.88722239781\n",
      "Step 193: Lower bound = -27725.88722239781\n",
      "Step 194: Lower bound = -27725.88722239781\n",
      "Step 195: Lower bound = -27725.88722239781\n",
      "Step 196: Lower bound = -27725.88722239781\n",
      "Step 197: Lower bound = -27725.88722239781\n",
      "Step 198: Lower bound = -27725.88722239781\n",
      "Step 199: Lower bound = -27725.88722239781\n",
      "Step 200: Lower bound = -27725.88722239781\n",
      "Step 201: Lower bound = -27725.88722239781\n",
      "Step 202: Lower bound = -27725.88722239781\n",
      "Step 203: Lower bound = -27725.88722239781\n",
      "Step 204: Lower bound = -27725.88722239781\n",
      "Step 205: Lower bound = -27725.88722239781\n",
      "Step 206: Lower bound = -27725.88722239781\n",
      "Step 207: Lower bound = -27725.887222397814\n",
      "Step 208: Lower bound = -27725.88722239781\n",
      "Step 209: Lower bound = -27725.887222397814\n",
      "Step 210: Lower bound = -27725.887222397814\n",
      "Step 211: Lower bound = -27725.887222397814\n",
      "Step 212: Lower bound = -27725.887222397814\n",
      "Step 213: Lower bound = -27725.887222397814\n",
      "Step 214: Lower bound = -27725.88722239781\n",
      "Step 215: Lower bound = -27725.887222397814\n",
      "Step 216: Lower bound = -27725.887222397814\n",
      "Step 217: Lower bound = -27725.887222397814\n",
      "Step 218: Lower bound = -27725.887222397814\n",
      "Step 219: Lower bound = -27725.887222397814\n",
      "Step 220: Lower bound = -27725.887222397814\n",
      "Step 221: Lower bound = -27725.88722239781\n",
      "Step 222: Lower bound = -27725.88722239781\n",
      "Step 223: Lower bound = -27725.887222397814\n",
      "Step 224: Lower bound = -27725.887222397814\n",
      "Step 225: Lower bound = -27725.887222397818\n",
      "Step 226: Lower bound = -27725.887222397814\n",
      "Step 227: Lower bound = -27725.887222397818\n",
      "Step 228: Lower bound = -27725.887222397818\n",
      "Step 229: Lower bound = -27725.887222397818\n",
      "Step 230: Lower bound = -27725.887222397814\n",
      "Step 231: Lower bound = -27725.887222397818\n",
      "Step 232: Lower bound = -27725.887222397814\n",
      "Step 233: Lower bound = -27725.887222397814\n",
      "Step 234: Lower bound = -27725.887222397814\n",
      "Step 235: Lower bound = -27725.887222397814\n",
      "Step 236: Lower bound = -27725.887222397818\n",
      "Step 237: Lower bound = -27725.887222397814\n",
      "Step 238: Lower bound = -27725.887222397814\n",
      "Step 239: Lower bound = -27725.887222397814\n",
      "Step 240: Lower bound = -27725.887222397814\n",
      "Step 241: Lower bound = -27725.887222397814\n",
      "Step 242: Lower bound = -27725.887222397814\n",
      "Step 243: Lower bound = -27725.887222397814\n",
      "Step 244: Lower bound = -27725.887222397814\n",
      "Step 245: Lower bound = -27725.887222397814\n",
      "Step 246: Lower bound = -27725.887222397814\n",
      "Step 247: Lower bound = -27725.887222397814\n",
      "Step 248: Lower bound = -27725.887222397814\n",
      "Step 249: Lower bound = -27725.887222397814\n",
      "Step 250: Lower bound = -27725.887222397814\n",
      "Step 251: Lower bound = -27725.887222397814\n",
      "Step 252: Lower bound = -27725.887222397814\n",
      "Step 253: Lower bound = -27725.887222397814\n",
      "Step 254: Lower bound = -27725.887222397814\n",
      "Step 255: Lower bound = -27725.887222397814\n",
      "Step 256: Lower bound = -27725.887222397814\n",
      "Step 257: Lower bound = -27725.887222397814\n",
      "Step 258: Lower bound = -27725.887222397814\n",
      "Step 259: Lower bound = -27725.887222397814\n",
      "Step 260: Lower bound = -27725.887222397814\n",
      "Step 261: Lower bound = -27725.887222397814\n",
      "Step 262: Lower bound = -27725.887222397814\n",
      "Step 263: Lower bound = -27725.887222397814\n",
      "Step 264: Lower bound = -27725.887222397814\n",
      "Step 265: Lower bound = -27725.887222397814\n",
      "Step 266: Lower bound = -27725.887222397814\n",
      "Step 267: Lower bound = -27725.887222397814\n",
      "Step 268: Lower bound = -27725.887222397814\n",
      "Step 269: Lower bound = -27725.887222397814\n",
      "Step 270: Lower bound = -27725.887222397814\n",
      "Step 271: Lower bound = -27725.887222397814\n",
      "Step 272: Lower bound = -27725.887222397814\n",
      "Step 273: Lower bound = -27725.887222397814\n",
      "Step 274: Lower bound = -27725.887222397814\n",
      "Step 275: Lower bound = -27725.887222397814\n",
      "Step 276: Lower bound = -27725.887222397814\n",
      "Step 277: Lower bound = -27725.887222397814\n",
      "Step 278: Lower bound = -27725.887222397814\n",
      "Step 279: Lower bound = -27725.887222397814\n",
      "Step 280: Lower bound = -27725.887222397814\n",
      "Step 281: Lower bound = -27725.887222397814\n",
      "Step 282: Lower bound = -27725.887222397814\n",
      "Step 283: Lower bound = -27725.887222397814\n",
      "Step 284: Lower bound = -27725.887222397814\n",
      "Step 285: Lower bound = -27725.887222397814\n",
      "Step 286: Lower bound = -27725.887222397814\n",
      "Step 287: Lower bound = -27725.887222397814\n",
      "Step 288: Lower bound = -27725.887222397814\n",
      "Step 289: Lower bound = -27725.887222397814\n",
      "Step 290: Lower bound = -27725.887222397814\n",
      "Step 291: Lower bound = -27725.887222397814\n",
      "Step 292: Lower bound = -27725.887222397814\n",
      "Step 293: Lower bound = -27725.887222397814\n",
      "Step 294: Lower bound = -27725.887222397814\n",
      "Step 295: Lower bound = -27725.887222397814\n",
      "Step 296: Lower bound = -27725.887222397814\n",
      "Step 297: Lower bound = -27725.887222397814\n",
      "Step 298: Lower bound = -27725.887222397814\n",
      "Step 299: Lower bound = -27725.887222397814\n",
      "Step 300: Lower bound = -27725.887222397814\n",
      "Step 301: Lower bound = -27725.887222397814\n",
      "Step 302: Lower bound = -27725.887222397814\n",
      "Step 303: Lower bound = -27725.887222397814\n",
      "Step 304: Lower bound = -27725.887222397814\n",
      "Step 305: Lower bound = -27725.887222397814\n",
      "Step 306: Lower bound = -27725.887222397814\n",
      "Step 307: Lower bound = -27725.887222397814\n",
      "Step 308: Lower bound = -27725.887222397814\n",
      "Step 309: Lower bound = -27725.887222397814\n",
      "Step 310: Lower bound = -27725.887222397814\n",
      "Step 311: Lower bound = -27725.887222397814\n",
      "Step 312: Lower bound = -27725.887222397814\n",
      "Step 313: Lower bound = -27725.887222397814\n",
      "Step 314: Lower bound = -27725.887222397814\n",
      "Step 315: Lower bound = -27725.887222397814\n",
      "Step 316: Lower bound = -27725.887222397814\n",
      "Step 317: Lower bound = -27725.887222397814\n",
      "Step 318: Lower bound = -27725.887222397814\n",
      "Step 319: Lower bound = -27725.887222397814\n",
      "Step 320: Lower bound = -27725.887222397814\n",
      "Step 321: Lower bound = -27725.887222397814\n",
      "Step 322: Lower bound = -27725.887222397814\n",
      "Step 323: Lower bound = -27725.887222397814\n",
      "Step 324: Lower bound = -27725.887222397814\n",
      "Step 325: Lower bound = -27725.887222397814\n",
      "Step 326: Lower bound = -27725.887222397814\n",
      "Step 327: Lower bound = -27725.887222397814\n",
      "Step 328: Lower bound = -27725.887222397814\n",
      "Step 329: Lower bound = -27725.887222397814\n",
      "Step 330: Lower bound = -27725.887222397814\n",
      "Step 331: Lower bound = -27725.887222397814\n",
      "Step 332: Lower bound = -27725.887222397814\n",
      "Step 333: Lower bound = -27725.887222397814\n",
      "Step 334: Lower bound = -27725.887222397814\n",
      "Step 335: Lower bound = -27725.887222397814\n",
      "Step 336: Lower bound = -27725.887222397814\n",
      "Step 337: Lower bound = -27725.887222397814\n",
      "Step 338: Lower bound = -27725.887222397814\n",
      "Step 339: Lower bound = -27725.887222397814\n",
      "Step 340: Lower bound = -27725.887222397814\n",
      "Step 341: Lower bound = -27725.887222397814\n",
      "Step 342: Lower bound = -27725.887222397814\n",
      "Step 343: Lower bound = -27725.887222397814\n",
      "Step 344: Lower bound = -27725.887222397814\n",
      "Step 345: Lower bound = -27725.887222397814\n",
      "Step 346: Lower bound = -27725.887222397814\n",
      "Step 347: Lower bound = -27725.887222397814\n",
      "Step 348: Lower bound = -27725.887222397814\n",
      "Step 349: Lower bound = -27725.887222397814\n",
      "Step 350: Lower bound = -27725.887222397814\n",
      "Step 351: Lower bound = -27725.887222397814\n",
      "Step 352: Lower bound = -27725.887222397814\n",
      "Step 353: Lower bound = -27725.887222397814\n",
      "Step 354: Lower bound = -27725.887222397814\n",
      "Step 355: Lower bound = -27725.887222397814\n",
      "Step 356: Lower bound = -27725.887222397814\n",
      "Step 357: Lower bound = -27725.887222397814\n",
      "Step 358: Lower bound = -27725.887222397814\n",
      "Step 359: Lower bound = -27725.887222397814\n",
      "Step 360: Lower bound = -27725.887222397814\n",
      "Step 361: Lower bound = -27725.887222397814\n",
      "Step 362: Lower bound = -27725.887222397814\n",
      "Step 363: Lower bound = -27725.887222397814\n",
      "Step 364: Lower bound = -27725.887222397814\n",
      "Step 365: Lower bound = -27725.887222397814\n",
      "Step 366: Lower bound = -27725.887222397814\n",
      "Step 367: Lower bound = -27725.887222397814\n",
      "Step 368: Lower bound = -27725.887222397814\n",
      "Step 369: Lower bound = -27725.887222397814\n",
      "Step 370: Lower bound = -27725.887222397814\n",
      "Step 371: Lower bound = -27725.887222397814\n",
      "Step 372: Lower bound = -27725.887222397814\n",
      "Step 373: Lower bound = -27725.887222397814\n",
      "Step 374: Lower bound = -27725.887222397814\n",
      "Step 375: Lower bound = -27725.887222397814\n",
      "Step 376: Lower bound = -27725.887222397814\n",
      "Step 377: Lower bound = -27725.887222397814\n",
      "Step 378: Lower bound = -27725.887222397814\n",
      "Step 379: Lower bound = -27725.887222397814\n",
      "Step 380: Lower bound = -27725.887222397814\n",
      "Step 381: Lower bound = -27725.887222397814\n",
      "Step 382: Lower bound = -27725.887222397814\n",
      "Step 383: Lower bound = -27725.887222397814\n",
      "Step 384: Lower bound = -27725.887222397814\n",
      "Step 385: Lower bound = -27725.887222397814\n",
      "Step 386: Lower bound = -27725.887222397814\n",
      "Step 387: Lower bound = -27725.887222397814\n",
      "Step 388: Lower bound = -27725.887222397814\n",
      "Step 389: Lower bound = -27725.887222397814\n",
      "Step 390: Lower bound = -27725.887222397814\n",
      "Step 391: Lower bound = -27725.887222397814\n",
      "Step 392: Lower bound = -27725.887222397814\n",
      "Step 393: Lower bound = -27725.887222397814\n",
      "Step 394: Lower bound = -27725.887222397814\n",
      "Step 395: Lower bound = -27725.887222397814\n",
      "Step 396: Lower bound = -27725.887222397814\n",
      "Step 397: Lower bound = -27725.887222397814\n",
      "Step 398: Lower bound = -27725.887222397814\n",
      "Step 399: Lower bound = -27725.887222397814\n",
      "Step 400: Lower bound = -27725.887222397814\n",
      "Step 401: Lower bound = -27725.887222397814\n",
      "Step 402: Lower bound = -27725.887222397814\n",
      "Step 403: Lower bound = -27725.887222397814\n",
      "Step 404: Lower bound = -27725.887222397814\n",
      "Step 405: Lower bound = -27725.887222397814\n",
      "Step 406: Lower bound = -27725.887222397814\n",
      "Step 407: Lower bound = -27725.887222397814\n",
      "Step 408: Lower bound = -27725.887222397814\n",
      "Step 409: Lower bound = -27725.887222397814\n",
      "Step 410: Lower bound = -27725.887222397814\n",
      "Step 411: Lower bound = -27725.887222397814\n",
      "Step 412: Lower bound = -27725.887222397814\n",
      "Step 413: Lower bound = -27725.887222397814\n",
      "Step 414: Lower bound = -27725.887222397814\n",
      "Step 415: Lower bound = -27725.887222397814\n",
      "Step 416: Lower bound = -27725.887222397814\n",
      "Step 417: Lower bound = -27725.887222397814\n",
      "Step 418: Lower bound = -27725.887222397814\n",
      "Step 419: Lower bound = -27725.887222397814\n",
      "Step 420: Lower bound = -27725.887222397814\n",
      "Step 421: Lower bound = -27725.887222397814\n",
      "Step 422: Lower bound = -27725.887222397814\n",
      "Step 423: Lower bound = -27725.887222397814\n",
      "Step 424: Lower bound = -27725.887222397814\n",
      "Step 425: Lower bound = -27725.887222397814\n",
      "Step 426: Lower bound = -27725.887222397814\n",
      "Step 427: Lower bound = -27725.887222397814\n",
      "Step 428: Lower bound = -27725.887222397814\n",
      "Step 429: Lower bound = -27725.887222397814\n",
      "Step 430: Lower bound = -27725.887222397814\n",
      "Step 431: Lower bound = -27725.887222397814\n",
      "Step 432: Lower bound = -27725.887222397814\n",
      "Step 433: Lower bound = -27725.887222397814\n",
      "Step 434: Lower bound = -27725.887222397814\n",
      "Step 435: Lower bound = -27725.887222397814\n",
      "Step 436: Lower bound = -27725.887222397814\n",
      "Step 437: Lower bound = -27725.887222397814\n",
      "Step 438: Lower bound = -27725.887222397814\n",
      "Step 439: Lower bound = -27725.887222397814\n",
      "Step 440: Lower bound = -27725.887222397814\n",
      "Step 441: Lower bound = -27725.887222397814\n",
      "Step 442: Lower bound = -27725.887222397814\n",
      "Step 443: Lower bound = -27725.887222397814\n",
      "Step 444: Lower bound = -27725.887222397814\n",
      "Step 445: Lower bound = -27725.887222397814\n",
      "Step 446: Lower bound = -27725.887222397814\n",
      "Step 447: Lower bound = -27725.887222397814\n",
      "Step 448: Lower bound = -27725.887222397814\n",
      "Step 449: Lower bound = -27725.887222397814\n",
      "Step 450: Lower bound = -27725.887222397814\n",
      "Step 451: Lower bound = -27725.887222397814\n",
      "Step 452: Lower bound = -27725.887222397814\n",
      "Step 453: Lower bound = -27725.887222397814\n",
      "Step 454: Lower bound = -27725.887222397814\n",
      "Step 455: Lower bound = -27725.887222397814\n",
      "Step 456: Lower bound = -27725.887222397814\n",
      "Step 457: Lower bound = -27725.887222397814\n",
      "Step 458: Lower bound = -27725.887222397814\n",
      "Step 459: Lower bound = -27725.887222397814\n",
      "Step 460: Lower bound = -27725.887222397814\n",
      "Step 461: Lower bound = -27725.887222397814\n",
      "Step 462: Lower bound = -27725.887222397814\n",
      "Step 463: Lower bound = -27725.887222397814\n",
      "Step 464: Lower bound = -27725.887222397814\n",
      "Step 465: Lower bound = -27725.887222397814\n",
      "Step 466: Lower bound = -27725.887222397814\n",
      "Step 467: Lower bound = -27725.887222397814\n",
      "Step 468: Lower bound = -27725.887222397814\n",
      "Step 469: Lower bound = -27725.887222397814\n",
      "Step 470: Lower bound = -27725.887222397814\n",
      "Step 471: Lower bound = -27725.887222397814\n",
      "Step 472: Lower bound = -27725.887222397814\n",
      "Step 473: Lower bound = -27725.887222397814\n",
      "Step 474: Lower bound = -27725.887222397814\n",
      "Step 475: Lower bound = -27725.887222397814\n",
      "Step 476: Lower bound = -27725.887222397814\n",
      "Step 477: Lower bound = -27725.887222397814\n",
      "Step 478: Lower bound = -27725.887222397814\n",
      "Step 479: Lower bound = -27725.887222397814\n",
      "Step 480: Lower bound = -27725.887222397814\n",
      "Step 481: Lower bound = -27725.887222397814\n",
      "Step 482: Lower bound = -27725.887222397814\n",
      "Step 483: Lower bound = -27725.887222397814\n",
      "Step 484: Lower bound = -27725.887222397814\n",
      "Step 485: Lower bound = -27725.887222397814\n",
      "Step 486: Lower bound = -27725.887222397814\n",
      "Step 487: Lower bound = -27725.887222397814\n",
      "Step 488: Lower bound = -27725.887222397814\n",
      "Step 489: Lower bound = -27725.887222397814\n",
      "Step 490: Lower bound = -27725.887222397814\n",
      "Step 491: Lower bound = -27725.887222397814\n",
      "Step 492: Lower bound = -27725.887222397814\n",
      "Step 493: Lower bound = -27725.887222397814\n",
      "Step 494: Lower bound = -27725.887222397814\n",
      "Step 495: Lower bound = -27725.887222397814\n",
      "Step 496: Lower bound = -27725.887222397814\n",
      "Step 497: Lower bound = -27725.887222397814\n",
      "Step 498: Lower bound = -27725.887222397814\n",
      "Step 499: Lower bound = -27725.887222397814\n",
      "Step 500: Lower bound = -27725.887222397814\n",
      "Step 501: Lower bound = -27725.887222397814\n",
      "Step 502: Lower bound = -27725.887222397814\n",
      "Step 503: Lower bound = -27725.887222397814\n",
      "Step 504: Lower bound = -27725.887222397814\n",
      "Step 505: Lower bound = -27725.887222397814\n",
      "Step 506: Lower bound = -27725.887222397814\n",
      "Step 507: Lower bound = -27725.887222397814\n",
      "Step 508: Lower bound = -27725.887222397814\n",
      "Step 509: Lower bound = -27725.887222397814\n",
      "Step 510: Lower bound = -27725.887222397814\n",
      "Step 511: Lower bound = -27725.887222397814\n",
      "Step 512: Lower bound = -27725.887222397814\n",
      "Step 513: Lower bound = -27725.887222397814\n",
      "Step 514: Lower bound = -27725.887222397814\n",
      "Step 515: Lower bound = -27725.887222397814\n",
      "Step 516: Lower bound = -27725.887222397814\n",
      "Step 517: Lower bound = -27725.887222397814\n",
      "Step 518: Lower bound = -27725.887222397814\n",
      "Step 519: Lower bound = -27725.887222397814\n",
      "Step 520: Lower bound = -27725.887222397814\n",
      "Step 521: Lower bound = -27725.887222397814\n",
      "Step 522: Lower bound = -27725.887222397814\n",
      "Step 523: Lower bound = -27725.887222397814\n",
      "Step 524: Lower bound = -27725.887222397814\n",
      "Step 525: Lower bound = -27725.887222397814\n",
      "Step 526: Lower bound = -27725.887222397814\n",
      "Step 527: Lower bound = -27725.887222397814\n",
      "Step 528: Lower bound = -27725.887222397814\n",
      "Step 529: Lower bound = -27725.887222397814\n",
      "Step 530: Lower bound = -27725.887222397814\n",
      "Step 531: Lower bound = -27725.887222397814\n",
      "Step 532: Lower bound = -27725.887222397814\n",
      "Step 533: Lower bound = -27725.887222397814\n",
      "Step 534: Lower bound = -27725.887222397814\n",
      "Step 535: Lower bound = -27725.887222397814\n",
      "Step 536: Lower bound = -27725.887222397814\n",
      "Step 537: Lower bound = -27725.887222397814\n",
      "Step 538: Lower bound = -27725.887222397814\n",
      "Step 539: Lower bound = -27725.887222397814\n",
      "Step 540: Lower bound = -27725.887222397814\n",
      "Step 541: Lower bound = -27725.887222397814\n",
      "Step 542: Lower bound = -27725.887222397814\n",
      "Step 543: Lower bound = -27725.887222397814\n",
      "Step 544: Lower bound = -27725.887222397814\n",
      "Step 545: Lower bound = -27725.887222397814\n",
      "Step 546: Lower bound = -27725.887222397814\n",
      "Step 547: Lower bound = -27725.887222397814\n",
      "Step 548: Lower bound = -27725.887222397814\n",
      "Step 549: Lower bound = -27725.887222397814\n",
      "Step 550: Lower bound = -27725.887222397814\n",
      "Step 551: Lower bound = -27725.887222397814\n",
      "Step 552: Lower bound = -27725.887222397814\n",
      "Step 553: Lower bound = -27725.887222397814\n",
      "Step 554: Lower bound = -27725.887222397814\n",
      "Step 555: Lower bound = -27725.887222397814\n",
      "Step 556: Lower bound = -27725.887222397814\n",
      "Step 557: Lower bound = -27725.887222397814\n",
      "Step 558: Lower bound = -27725.887222397814\n",
      "Step 559: Lower bound = -27725.887222397814\n",
      "Step 560: Lower bound = -27725.887222397814\n",
      "Step 561: Lower bound = -27725.887222397814\n",
      "Step 562: Lower bound = -27725.887222397814\n",
      "Step 563: Lower bound = -27725.887222397814\n",
      "Step 564: Lower bound = -27725.887222397814\n",
      "Step 565: Lower bound = -27725.887222397814\n",
      "Step 566: Lower bound = -27725.887222397814\n",
      "Step 567: Lower bound = -27725.887222397814\n",
      "Step 568: Lower bound = -27725.887222397814\n",
      "Step 569: Lower bound = -27725.887222397814\n",
      "Step 570: Lower bound = -27725.887222397814\n",
      "Step 571: Lower bound = -27725.887222397814\n",
      "Step 572: Lower bound = -27725.887222397814\n",
      "Step 573: Lower bound = -27725.887222397814\n",
      "Step 574: Lower bound = -27725.887222397814\n",
      "Step 575: Lower bound = -27725.887222397814\n",
      "Step 576: Lower bound = -27725.887222397814\n",
      "Step 577: Lower bound = -27725.887222397814\n",
      "Step 578: Lower bound = -27725.887222397814\n",
      "Step 579: Lower bound = -27725.887222397814\n",
      "Step 580: Lower bound = -27725.887222397814\n",
      "Step 581: Lower bound = -27725.887222397814\n",
      "Step 582: Lower bound = -27725.887222397814\n",
      "Step 583: Lower bound = -27725.887222397814\n",
      "Step 584: Lower bound = -27725.887222397814\n",
      "Step 585: Lower bound = -27725.887222397814\n",
      "Step 586: Lower bound = -27725.887222397814\n",
      "Step 587: Lower bound = -27725.887222397814\n",
      "Step 588: Lower bound = -27725.887222397814\n",
      "Step 589: Lower bound = -27725.887222397814\n",
      "Step 590: Lower bound = -27725.887222397814\n",
      "Step 591: Lower bound = -27725.887222397814\n",
      "Step 592: Lower bound = -27725.887222397814\n",
      "Step 593: Lower bound = -27725.887222397814\n",
      "Step 594: Lower bound = -27725.887222397814\n",
      "Step 595: Lower bound = -27725.887222397814\n",
      "Step 596: Lower bound = -27725.887222397814\n",
      "Step 597: Lower bound = -27725.887222397814\n",
      "Step 598: Lower bound = -27725.887222397814\n",
      "Step 599: Lower bound = -27725.887222397814\n",
      "Step 600: Lower bound = -27725.887222397814\n",
      "Step 601: Lower bound = -27725.887222397814\n",
      "Step 602: Lower bound = -27725.887222397814\n",
      "Step 603: Lower bound = -27725.887222397814\n",
      "Step 604: Lower bound = -27725.887222397814\n",
      "Step 605: Lower bound = -27725.887222397814\n",
      "Step 606: Lower bound = -27725.887222397814\n",
      "Step 607: Lower bound = -27725.887222397814\n",
      "Step 608: Lower bound = -27725.887222397814\n",
      "Step 609: Lower bound = -27725.887222397814\n",
      "Step 610: Lower bound = -27725.887222397814\n",
      "Step 611: Lower bound = -27725.887222397814\n",
      "Step 612: Lower bound = -27725.887222397814\n",
      "Step 613: Lower bound = -27725.887222397814\n",
      "Step 614: Lower bound = -27725.887222397814\n",
      "Step 615: Lower bound = -27725.887222397814\n",
      "Step 616: Lower bound = -27725.887222397814\n",
      "Step 617: Lower bound = -27725.887222397814\n",
      "Step 618: Lower bound = -27725.887222397814\n",
      "Step 619: Lower bound = -27725.887222397814\n",
      "Step 620: Lower bound = -27725.887222397814\n",
      "Step 621: Lower bound = -27725.887222397814\n",
      "Step 622: Lower bound = -27725.887222397814\n",
      "Step 623: Lower bound = -27725.887222397814\n",
      "Step 624: Lower bound = -27725.887222397814\n",
      "Step 625: Lower bound = -27725.887222397814\n",
      "Step 626: Lower bound = -27725.887222397814\n",
      "Step 627: Lower bound = -27725.887222397814\n",
      "Step 628: Lower bound = -27725.887222397814\n",
      "Step 629: Lower bound = -27725.887222397814\n",
      "Step 630: Lower bound = -27725.887222397814\n",
      "Step 631: Lower bound = -27725.887222397814\n",
      "Step 632: Lower bound = -27725.887222397814\n",
      "Step 633: Lower bound = -27725.887222397814\n",
      "Step 634: Lower bound = -27725.887222397814\n",
      "Step 635: Lower bound = -27725.887222397814\n",
      "Step 636: Lower bound = -27725.887222397814\n",
      "Step 637: Lower bound = -27725.887222397814\n",
      "Step 638: Lower bound = -27725.887222397814\n",
      "Step 639: Lower bound = -27725.887222397814\n",
      "Step 640: Lower bound = -27725.887222397814\n",
      "Step 641: Lower bound = -27725.887222397814\n",
      "Step 642: Lower bound = -27725.887222397814\n",
      "Step 643: Lower bound = -27725.887222397814\n",
      "Step 644: Lower bound = -27725.887222397814\n",
      "Step 645: Lower bound = -27725.887222397814\n",
      "Step 646: Lower bound = -27725.887222397814\n",
      "Step 647: Lower bound = -27725.887222397814\n",
      "Step 648: Lower bound = -27725.887222397814\n",
      "Step 649: Lower bound = -27725.887222397814\n",
      "Step 650: Lower bound = -27725.887222397814\n",
      "Step 651: Lower bound = -27725.887222397814\n",
      "Step 652: Lower bound = -27725.887222397814\n",
      "Step 653: Lower bound = -27725.887222397814\n",
      "Step 654: Lower bound = -27725.887222397814\n",
      "Step 655: Lower bound = -27725.887222397814\n",
      "Step 656: Lower bound = -27725.887222397814\n",
      "Step 657: Lower bound = -27725.887222397814\n",
      "Step 658: Lower bound = -27725.887222397814\n",
      "Step 659: Lower bound = -27725.887222397814\n",
      "Step 660: Lower bound = -27725.887222397814\n",
      "Step 661: Lower bound = -27725.887222397814\n",
      "Step 662: Lower bound = -27725.887222397814\n",
      "Step 663: Lower bound = -27725.887222397814\n",
      "Step 664: Lower bound = -27725.887222397814\n",
      "Step 665: Lower bound = -27725.887222397814\n",
      "Step 666: Lower bound = -27725.887222397814\n",
      "Step 667: Lower bound = -27725.887222397814\n",
      "Step 668: Lower bound = -27725.887222397814\n",
      "Step 669: Lower bound = -27725.887222397814\n",
      "Step 670: Lower bound = -27725.887222397814\n",
      "Step 671: Lower bound = -27725.887222397814\n",
      "Step 672: Lower bound = -27725.887222397814\n",
      "Step 673: Lower bound = -27725.887222397814\n",
      "Step 674: Lower bound = -27725.887222397814\n",
      "Step 675: Lower bound = -27725.887222397814\n",
      "Step 676: Lower bound = -27725.887222397814\n",
      "Step 677: Lower bound = -27725.887222397814\n",
      "Step 678: Lower bound = -27725.887222397814\n",
      "Step 679: Lower bound = -27725.887222397814\n",
      "Step 680: Lower bound = -27725.887222397814\n",
      "Step 681: Lower bound = -27725.887222397814\n",
      "Step 682: Lower bound = -27725.887222397814\n",
      "Step 683: Lower bound = -27725.887222397814\n",
      "Step 684: Lower bound = -27725.887222397814\n",
      "Step 685: Lower bound = -27725.887222397814\n",
      "Step 686: Lower bound = -27725.887222397814\n",
      "Step 687: Lower bound = -27725.887222397814\n",
      "Step 688: Lower bound = -27725.887222397814\n",
      "Step 689: Lower bound = -27725.887222397814\n",
      "Step 690: Lower bound = -27725.887222397814\n",
      "Step 691: Lower bound = -27725.887222397814\n",
      "Step 692: Lower bound = -27725.887222397814\n",
      "Step 693: Lower bound = -27725.887222397814\n",
      "Step 694: Lower bound = -27725.887222397814\n",
      "Step 695: Lower bound = -27725.887222397814\n",
      "Step 696: Lower bound = -27725.887222397814\n",
      "Step 697: Lower bound = -27725.887222397814\n",
      "Step 698: Lower bound = -27725.887222397814\n",
      "Step 699: Lower bound = -27725.887222397814\n",
      "Step 700: Lower bound = -27725.887222397814\n",
      "Step 701: Lower bound = -27725.887222397814\n",
      "Step 702: Lower bound = -27725.887222397814\n",
      "Step 703: Lower bound = -27725.887222397814\n",
      "Step 704: Lower bound = -27725.887222397814\n",
      "Step 705: Lower bound = -27725.887222397814\n",
      "Step 706: Lower bound = -27725.887222397814\n",
      "Step 707: Lower bound = -27725.887222397814\n",
      "Step 708: Lower bound = -27725.887222397814\n",
      "Step 709: Lower bound = -27725.887222397814\n",
      "Step 710: Lower bound = -27725.887222397814\n",
      "Step 711: Lower bound = -27725.887222397814\n",
      "Step 712: Lower bound = -27725.887222397814\n",
      "Step 713: Lower bound = -27725.887222397814\n",
      "Step 714: Lower bound = -27725.887222397814\n",
      "Step 715: Lower bound = -27725.887222397814\n",
      "Step 716: Lower bound = -27725.887222397814\n",
      "Step 717: Lower bound = -27725.887222397814\n",
      "Step 718: Lower bound = -27725.887222397814\n",
      "Step 719: Lower bound = -27725.887222397814\n",
      "Step 720: Lower bound = -27725.887222397814\n",
      "Step 721: Lower bound = -27725.887222397814\n",
      "Step 722: Lower bound = -27725.887222397814\n",
      "Step 723: Lower bound = -27725.887222397814\n",
      "Step 724: Lower bound = -27725.887222397814\n",
      "Step 725: Lower bound = -27725.887222397814\n",
      "Step 726: Lower bound = -27725.887222397814\n",
      "Step 727: Lower bound = -27725.887222397814\n",
      "Step 728: Lower bound = -27725.887222397814\n",
      "Step 729: Lower bound = -27725.887222397814\n",
      "Step 730: Lower bound = -27725.887222397814\n",
      "Step 731: Lower bound = -27725.887222397814\n",
      "Step 732: Lower bound = -27725.887222397814\n",
      "Step 733: Lower bound = -27725.887222397814\n",
      "Step 734: Lower bound = -27725.887222397814\n",
      "Step 735: Lower bound = -27725.887222397814\n",
      "Step 736: Lower bound = -27725.887222397814\n",
      "Step 737: Lower bound = -27725.887222397814\n",
      "Step 738: Lower bound = -27725.887222397814\n",
      "Step 739: Lower bound = -27725.887222397814\n",
      "Step 740: Lower bound = -27725.887222397814\n",
      "Step 741: Lower bound = -27725.887222397814\n",
      "Step 742: Lower bound = -27725.887222397814\n",
      "Step 743: Lower bound = -27725.887222397814\n",
      "Step 744: Lower bound = -27725.887222397814\n",
      "Step 745: Lower bound = -27725.887222397814\n",
      "Step 746: Lower bound = -27725.887222397814\n",
      "Step 747: Lower bound = -27725.887222397814\n",
      "Step 748: Lower bound = -27725.887222397814\n",
      "Step 749: Lower bound = -27725.887222397814\n",
      "Step 750: Lower bound = -27725.887222397814\n",
      "Step 751: Lower bound = -27725.887222397814\n",
      "Step 752: Lower bound = -27725.887222397814\n",
      "Step 753: Lower bound = -27725.887222397814\n",
      "Step 754: Lower bound = -27725.887222397814\n",
      "Step 755: Lower bound = -27725.887222397814\n",
      "Step 756: Lower bound = -27725.887222397814\n",
      "Step 757: Lower bound = -27725.887222397814\n",
      "Step 758: Lower bound = -27725.887222397814\n",
      "Step 759: Lower bound = -27725.887222397814\n",
      "Step 760: Lower bound = -27725.887222397814\n",
      "Step 761: Lower bound = -27725.887222397814\n",
      "Step 762: Lower bound = -27725.887222397814\n",
      "Step 763: Lower bound = -27725.887222397814\n",
      "Step 764: Lower bound = -27725.887222397814\n",
      "Step 765: Lower bound = -27725.887222397814\n",
      "Step 766: Lower bound = -27725.887222397814\n",
      "Step 767: Lower bound = -27725.887222397814\n",
      "Step 768: Lower bound = -27725.887222397814\n",
      "Step 769: Lower bound = -27725.887222397814\n",
      "Step 770: Lower bound = -27725.887222397814\n",
      "Step 771: Lower bound = -27725.887222397814\n",
      "Step 772: Lower bound = -27725.887222397814\n",
      "Step 773: Lower bound = -27725.887222397814\n",
      "Step 774: Lower bound = -27725.887222397814\n",
      "Step 775: Lower bound = -27725.887222397814\n",
      "Step 776: Lower bound = -27725.887222397814\n",
      "Step 777: Lower bound = -27725.887222397814\n",
      "Step 778: Lower bound = -27725.887222397814\n",
      "Step 779: Lower bound = -27725.887222397814\n",
      "Step 780: Lower bound = -27725.887222397814\n",
      "Step 781: Lower bound = -27725.887222397814\n",
      "Step 782: Lower bound = -27725.887222397814\n",
      "Step 783: Lower bound = -27725.887222397814\n",
      "Step 784: Lower bound = -27725.887222397814\n",
      "Step 785: Lower bound = -27725.887222397814\n",
      "Step 786: Lower bound = -27725.887222397814\n",
      "Step 787: Lower bound = -27725.887222397814\n",
      "Step 788: Lower bound = -27725.887222397814\n",
      "Step 789: Lower bound = -27725.887222397814\n",
      "Step 790: Lower bound = -27725.887222397814\n",
      "Step 791: Lower bound = -27725.887222397814\n",
      "Step 792: Lower bound = -27725.887222397814\n",
      "Step 793: Lower bound = -27725.887222397814\n",
      "Step 794: Lower bound = -27725.887222397814\n",
      "Step 795: Lower bound = -27725.887222397814\n",
      "Step 796: Lower bound = -27725.887222397814\n",
      "Step 797: Lower bound = -27725.887222397814\n",
      "Step 798: Lower bound = -27725.887222397814\n",
      "Step 799: Lower bound = -27725.887222397814\n",
      "Step 800: Lower bound = -27725.887222397814\n",
      "Step 801: Lower bound = -27725.887222397814\n",
      "Step 802: Lower bound = -27725.887222397814\n",
      "Step 803: Lower bound = -27725.887222397814\n",
      "Step 804: Lower bound = -27725.887222397814\n",
      "Step 805: Lower bound = -27725.887222397814\n",
      "Step 806: Lower bound = -27725.887222397814\n",
      "Step 807: Lower bound = -27725.887222397814\n",
      "Step 808: Lower bound = -27725.887222397814\n",
      "Step 809: Lower bound = -27725.887222397814\n",
      "Step 810: Lower bound = -27725.887222397814\n",
      "Step 811: Lower bound = -27725.887222397814\n",
      "Step 812: Lower bound = -27725.887222397814\n",
      "Step 813: Lower bound = -27725.887222397814\n",
      "Step 814: Lower bound = -27725.887222397814\n",
      "Step 815: Lower bound = -27725.887222397814\n",
      "Step 816: Lower bound = -27725.887222397814\n",
      "Step 817: Lower bound = -27725.887222397814\n",
      "Step 818: Lower bound = -27725.887222397814\n",
      "Step 819: Lower bound = -27725.887222397814\n",
      "Step 820: Lower bound = -27725.887222397814\n",
      "Step 821: Lower bound = -27725.887222397814\n",
      "Step 822: Lower bound = -27725.887222397814\n",
      "Step 823: Lower bound = -27725.887222397814\n",
      "Step 824: Lower bound = -27725.887222397814\n",
      "Step 825: Lower bound = -27725.887222397814\n",
      "Step 826: Lower bound = -27725.887222397814\n",
      "Step 827: Lower bound = -27725.887222397814\n",
      "Step 828: Lower bound = -27725.887222397814\n",
      "Step 829: Lower bound = -27725.887222397814\n",
      "Step 830: Lower bound = -27725.887222397814\n",
      "Step 831: Lower bound = -27725.887222397814\n",
      "Step 832: Lower bound = -27725.887222397814\n",
      "Step 833: Lower bound = -27725.887222397814\n",
      "Step 834: Lower bound = -27725.887222397814\n",
      "Step 835: Lower bound = -27725.887222397814\n",
      "Step 836: Lower bound = -27725.887222397814\n",
      "Step 837: Lower bound = -27725.887222397814\n",
      "Step 838: Lower bound = -27725.887222397814\n",
      "Step 839: Lower bound = -27725.887222397814\n",
      "Step 840: Lower bound = -27725.887222397814\n",
      "Step 841: Lower bound = -27725.887222397814\n",
      "Step 842: Lower bound = -27725.887222397814\n",
      "Step 843: Lower bound = -27725.887222397814\n",
      "Step 844: Lower bound = -27725.887222397814\n",
      "Step 845: Lower bound = -27725.887222397814\n",
      "Step 846: Lower bound = -27725.887222397814\n",
      "Step 847: Lower bound = -27725.887222397814\n",
      "Step 848: Lower bound = -27725.887222397814\n",
      "Step 849: Lower bound = -27725.887222397814\n",
      "Step 850: Lower bound = -27725.887222397814\n",
      "Step 851: Lower bound = -27725.887222397814\n",
      "Step 852: Lower bound = -27725.887222397814\n",
      "Step 853: Lower bound = -27725.887222397814\n",
      "Step 854: Lower bound = -27725.887222397814\n",
      "Step 855: Lower bound = -27725.887222397814\n",
      "Step 856: Lower bound = -27725.887222397814\n",
      "Step 857: Lower bound = -27725.887222397814\n",
      "Step 858: Lower bound = -27725.887222397814\n",
      "Step 859: Lower bound = -27725.887222397814\n",
      "Step 860: Lower bound = -27725.887222397814\n",
      "Step 861: Lower bound = -27725.887222397814\n",
      "Step 862: Lower bound = -27725.887222397814\n",
      "Step 863: Lower bound = -27725.887222397814\n",
      "Step 864: Lower bound = -27725.887222397814\n",
      "Step 865: Lower bound = -27725.887222397814\n",
      "Step 866: Lower bound = -27725.887222397814\n",
      "Step 867: Lower bound = -27725.887222397814\n",
      "Step 868: Lower bound = -27725.887222397814\n",
      "Step 869: Lower bound = -27725.887222397814\n",
      "Step 870: Lower bound = -27725.887222397814\n",
      "Step 871: Lower bound = -27725.887222397814\n",
      "Step 872: Lower bound = -27725.887222397814\n",
      "Step 873: Lower bound = -27725.887222397814\n",
      "Step 874: Lower bound = -27725.887222397814\n",
      "Step 875: Lower bound = -27725.887222397814\n",
      "Step 876: Lower bound = -27725.887222397814\n",
      "Step 877: Lower bound = -27725.887222397814\n",
      "Step 878: Lower bound = -27725.887222397814\n",
      "Step 879: Lower bound = -27725.887222397814\n",
      "Step 880: Lower bound = -27725.887222397814\n",
      "Step 881: Lower bound = -27725.887222397814\n",
      "Step 882: Lower bound = -27725.887222397814\n",
      "Step 883: Lower bound = -27725.887222397814\n",
      "Step 884: Lower bound = -27725.887222397814\n",
      "Step 885: Lower bound = -27725.887222397814\n",
      "Step 886: Lower bound = -27725.887222397814\n",
      "Step 887: Lower bound = -27725.887222397814\n",
      "Step 888: Lower bound = -27725.887222397814\n",
      "Step 889: Lower bound = -27725.887222397814\n",
      "Step 890: Lower bound = -27725.887222397814\n",
      "Step 891: Lower bound = -27725.887222397814\n",
      "Step 892: Lower bound = -27725.887222397814\n",
      "Step 893: Lower bound = -27725.887222397814\n",
      "Step 894: Lower bound = -27725.887222397814\n",
      "Step 895: Lower bound = -27725.887222397814\n",
      "Step 896: Lower bound = -27725.887222397814\n",
      "Step 897: Lower bound = -27725.887222397814\n",
      "Step 898: Lower bound = -27725.887222397814\n",
      "Step 899: Lower bound = -27725.887222397814\n",
      "Step 900: Lower bound = -27725.887222397814\n",
      "Step 901: Lower bound = -27725.887222397814\n",
      "Step 902: Lower bound = -27725.887222397814\n",
      "Step 903: Lower bound = -27725.887222397814\n",
      "Step 904: Lower bound = -27725.887222397814\n",
      "Step 905: Lower bound = -27725.887222397814\n",
      "Step 906: Lower bound = -27725.887222397814\n",
      "Step 907: Lower bound = -27725.887222397814\n",
      "Step 908: Lower bound = -27725.887222397814\n",
      "Step 909: Lower bound = -27725.887222397814\n",
      "Step 910: Lower bound = -27725.887222397814\n",
      "Step 911: Lower bound = -27725.887222397814\n",
      "Step 912: Lower bound = -27725.887222397814\n",
      "Step 913: Lower bound = -27725.887222397814\n",
      "Step 914: Lower bound = -27725.887222397814\n",
      "Step 915: Lower bound = -27725.887222397814\n",
      "Step 916: Lower bound = -27725.887222397814\n",
      "Step 917: Lower bound = -27725.887222397814\n",
      "Step 918: Lower bound = -27725.887222397814\n",
      "Step 919: Lower bound = -27725.887222397814\n",
      "Step 920: Lower bound = -27725.887222397814\n",
      "Step 921: Lower bound = -27725.887222397814\n",
      "Step 922: Lower bound = -27725.887222397814\n",
      "Step 923: Lower bound = -27725.887222397814\n",
      "Step 924: Lower bound = -27725.887222397814\n",
      "Step 925: Lower bound = -27725.887222397814\n",
      "Step 926: Lower bound = -27725.887222397814\n",
      "Step 927: Lower bound = -27725.887222397814\n",
      "Step 928: Lower bound = -27725.887222397814\n",
      "Step 929: Lower bound = -27725.887222397814\n",
      "Step 930: Lower bound = -27725.887222397814\n",
      "Step 931: Lower bound = -27725.887222397814\n",
      "Step 932: Lower bound = -27725.887222397814\n",
      "Step 933: Lower bound = -27725.887222397814\n",
      "Step 934: Lower bound = -27725.887222397814\n",
      "Step 935: Lower bound = -27725.887222397814\n",
      "Step 936: Lower bound = -27725.887222397814\n",
      "Step 937: Lower bound = -27725.887222397814\n",
      "Step 938: Lower bound = -27725.887222397814\n",
      "Step 939: Lower bound = -27725.887222397814\n",
      "Step 940: Lower bound = -27725.887222397814\n",
      "Step 941: Lower bound = -27725.887222397814\n",
      "Step 942: Lower bound = -27725.887222397814\n",
      "Step 943: Lower bound = -27725.887222397814\n",
      "Step 944: Lower bound = -27725.887222397814\n",
      "Step 945: Lower bound = -27725.887222397814\n",
      "Step 946: Lower bound = -27725.887222397814\n",
      "Step 947: Lower bound = -27725.887222397814\n",
      "Step 948: Lower bound = -27725.887222397814\n",
      "Step 949: Lower bound = -27725.887222397814\n",
      "Step 950: Lower bound = -27725.887222397814\n",
      "Step 951: Lower bound = -27725.887222397814\n",
      "Step 952: Lower bound = -27725.887222397814\n",
      "Step 953: Lower bound = -27725.887222397814\n",
      "Step 954: Lower bound = -27725.887222397814\n",
      "Step 955: Lower bound = -27725.887222397814\n",
      "Step 956: Lower bound = -27725.887222397814\n",
      "Step 957: Lower bound = -27725.887222397814\n",
      "Step 958: Lower bound = -27725.887222397814\n",
      "Step 959: Lower bound = -27725.887222397814\n",
      "Step 960: Lower bound = -27725.887222397814\n",
      "Step 961: Lower bound = -27725.887222397814\n",
      "Step 962: Lower bound = -27725.887222397814\n",
      "Step 963: Lower bound = -27725.887222397814\n",
      "Step 964: Lower bound = -27725.887222397814\n",
      "Step 965: Lower bound = -27725.887222397814\n",
      "Step 966: Lower bound = -27725.887222397814\n",
      "Step 967: Lower bound = -27725.887222397814\n",
      "Step 968: Lower bound = -27725.887222397814\n",
      "Step 969: Lower bound = -27725.887222397814\n",
      "Step 970: Lower bound = -27725.887222397814\n",
      "Step 971: Lower bound = -27725.887222397814\n",
      "Step 972: Lower bound = -27725.887222397814\n",
      "Step 973: Lower bound = -27725.887222397814\n",
      "Step 974: Lower bound = -27725.887222397814\n",
      "Step 975: Lower bound = -27725.887222397814\n",
      "Step 976: Lower bound = -27725.887222397814\n",
      "Step 977: Lower bound = -27725.887222397814\n",
      "Step 978: Lower bound = -27725.887222397814\n",
      "Step 979: Lower bound = -27725.887222397814\n",
      "Step 980: Lower bound = -27725.887222397814\n",
      "Step 981: Lower bound = -27725.887222397814\n",
      "Step 982: Lower bound = -27725.887222397814\n",
      "Step 983: Lower bound = -27725.887222397814\n",
      "Step 984: Lower bound = -27725.887222397814\n",
      "Step 985: Lower bound = -27725.887222397814\n",
      "Step 986: Lower bound = -27725.887222397814\n",
      "Step 987: Lower bound = -27725.887222397814\n",
      "Step 988: Lower bound = -27725.887222397814\n",
      "Step 989: Lower bound = -27725.887222397814\n",
      "Step 990: Lower bound = -27725.887222397814\n",
      "Step 991: Lower bound = -27725.887222397814\n",
      "Step 992: Lower bound = -27725.887222397814\n",
      "Step 993: Lower bound = -27725.887222397814\n",
      "Step 994: Lower bound = -27725.887222397814\n",
      "Step 995: Lower bound = -27725.887222397814\n",
      "Step 996: Lower bound = -27725.887222397814\n",
      "Step 997: Lower bound = -27725.887222397814\n",
      "Step 998: Lower bound = -27725.887222397814\n",
      "Step 999: Lower bound = -27725.887222397814\n"
     ]
    }
   ],
   "source": [
    "alpha_em, logbeta_em, q_em, lower_bound_list = em(L=L, n_steps=1000, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(np.arange(1, len(lower_bound_list) + 1), lower_bound_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Задание 2. (1 балл)** Загрузите настоящую разметку. Посчитайте `accuracy` разметки, полученной с помощью обычного голосования по большинству среди экспертов, и сравните его с качеством, полученным с помощью EM-алгоритма. Помните, что алгоритму не важно, какая метка 0, а какая 1, поэтому если получите качество <0.5, то просто поменяйте метки классов (не забудьте также поменять знак у $\\alpha$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y = np.load('y.npy')\n",
    "from scipy.stats import mode\n",
    "\n",
    "# (∩ ￣ー￣)⊃ ✳✨✳✨✳✨✳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# предсказания по большинству среди экспертов\n",
    "majority_voting_predictions = mode(L, axis=1)[0]\n",
    "voting_accuracy = np.mean(majority_voting_predictions == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
      " [0.5 0.5 0.5 ... 0.5 0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "print(q_em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выбор класса с наибольшей вероятностью\n",
    "em_predictions = np.argmax(q_em, axis=0)\n",
    "em_accuracy = np.mean(em_predictions == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy с помощью голосования по большинству среди экспертов: 0.904\n",
      "Accuracy с помощью EM-алгоритма: 0.5045\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy с помощью голосования по большинству среди экспертов:\", voting_accuracy)\n",
    "print(\"Accuracy с помощью EM-алгоритма:\", em_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Задание 3. (0.5 балла)** Попробуйте проинтерпретировать полученные коэфициенты $\\alpha$. Есть ли в выборке эксперты, которые намеренно голосуют неверно? Как это можно понять по альфам? Продемонстрируйте, что эксперты действительно чаще голосуют за неверный класс. Постройте график зависимости доли врено размеченных экспертом объектов от коэффициента $\\alpha$. Прокомментируйте результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# (∩ᄑ_ᄑ)⊃━☆ﾟ*･｡*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Задание 4. (бонус, 2 балла)**  Как уже было замечено выше, модели не важно, какой класс 1, а какой 0. Скажем, если все эксперты оказались максимально противными и ставят метку с точностью наоборот, то у вас будет полная согласованность между экспертами, при этом невозможно понять правильно они разметили выборку или нет, смотря только на такую разметку. Чтобы избежать этого, можно включать в выборку вопрос с заведомо известным ответом, тогда вы сможете определить, ставит ли эксперт специально неверные метки.\n",
    "\n",
    "Чтобы обощить данную модель на случай заданий с заведомо известной меткой, достоточно не делать для них E-шаг, а всегда полагать апостериорное распределение вырожденным в истинном классе. Реализуйте данную модель и используйте истинную разметку *для нескольких* задач из обучения. Проинтерпретируйте полученные результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Выравнивание слов (Word Alignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "EM-алгоритм также применяют на практике для настройки параметров модели выравнивания слов, более сложные модификации которой используются в статистическом машинном переводе. Мы не будем подробно обсуждать применение word alignment для перевода и ограничимся следующей целью: пусть у нас есть параллельный корпус из предложений на исходном языке и их переводов на целевой язык (в этом задании используются английский и чешский соответственно). \n",
    "\n",
    "Первая задача — определить с помощью этого корпуса, как переводится каждое отдельное слово на целевом языке. Вторая задача — для произвольной пары из предложения и его перевода установить, переводом какого слова в исходном предложении является каждое слово в целевом предложении. Оказывается, у обеих задач существует элегантное и эффективное решение при введении правильной вероятностной модели: в этой части задания вам предстоит его реализовать и оценить результаты работы. Но обо всём по порядку :)\n",
    "\n",
    "---\n",
    "\n",
    "Перед тем, как заниматься машинным обучением, давайте разберёмся с данными и метриками в интересующей нас задаче. В ячейке ниже загружается и разархивируется параллельный английско-чешский корпус, в котором есть разметка выравнивания слов. Нетрудно заметить, что формат XML-файла, использованный его авторами, не вполне стандартный: нет готовой команды , которая позволила бы получить список пар предложений вместе с выравниваниями. Это значит, что нужно разобраться с форматом и написать парсер самостоятельно, используя встроенные средства Python, например, модуль [xml](https://docs.python.org/3.7/library/xml.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget -q https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-1804/CzEnAli_1.0.tar.gz -O CzEnAli_1.0.tar.gz\n",
    "mkdir -p data\n",
    "tar -xzf CzEnAli_1.0.tar.gz -C data/\n",
    "head -n 20 data/merged_data/project_syndicate/project_syndicate_bacchetta1.wa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Задание -2. (0.5 балла)** Реализуйте функцию `extract_sentences`, которая принимает на вход путь к файлу с XML-разметкой, используемой в этом датасете, и возвращает список параллельных предложений, а также список из «уверенных» (sure) и «возможных» (possible) пар выравниваний. Отправьте вашу реализацию в Яндекс.Контест, чтобы убедиться в её корректности; в следующей ячейке ноутбука соберите все пары размеченных предложений из датасета в два списка `all_sentences` (список `SentencePair`) и `all_targets` (список LabeledAlignment).\n",
    "\n",
    "Здесь и далее соблюдайте сигнатуры функций и пользуйтесь объявленными в модуле `preprocessing.py` классами для организации данных. Стоит заметить, что предложения уже токенизированы (даже отделена пунктуация), поэтому предобработку текстов совершать не нужно. Обратите внимание на формат хранения выравниваний: нумерация начинается с 1 (в таком виде и нужно сохранять), первым в паре идёт слово из англоязычного предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from preprocessing import extract_sentences\n",
    "\n",
    "all_sentences = []\n",
    "all_targets = []\n",
    "# (´◕▽◕)⊃━☆"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Задание -1. (0.5 балла)** Реализуйте функции `get_token_to_index` и `tokenize_sents` из модуля `preprocessing.py`, постройте словари token->index для обоих языков и постройте список из `TokenizedSentencePair` по выборке. Реализации функций также отправьте в Яндекс.Контест."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from preprocessing import get_token_to_index, tokenize_sents\n",
    "\n",
    "t_idx_src, t_idx_tgt = get_token_to_index(all_sentences)\n",
    "tokenized_sentences = tokenize_sents(all_sentences, t_idx_src, t_idx_tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "В качестве бейзлайна для этой задачи мы возьмём способ выравнивания слов по коэффициенту Дайса: слово в исходном языке является переводом слова на целевом языке, если они часто встречаются в одних и тех же предложениях и редко встречаются по отдельности. \n",
    "\n",
    "Математически это записывается по аналогии с мерой Жаккара: пусть $c(x,y)$ — число параллельных предложений, в которых есть и $x$ (на исходном языке), и $y$ (на целевом языке), а $c(x)$ и $c(y)$ — суммарное количество предложений, в которых встречается слово $x$ и $y$ соответственно. Тогда $\\textrm{Dice}(x,y)=\\frac{2 \\cdot c(x,y)}{c(x) + c(y)}$ — характеристика «похожести» слов $x$ и $y$. Она равна 1, если слова встречаются только в контексте друг друга (не бывает предложений только со словом $x$ без $y$ в переводе и наоборот), равна 0, если слова никогда не встречаются в параллельных предложениях и находится между пороговыми значениями в остальных случаях.\n",
    "\n",
    "В файле `models.py` описан абстрактный класс `BaseAligner`, наследником которого должны являться все модели в задании, а также приведён пример реализации `DiceAligner` выравнивания слов описанным выше путём. Ниже вы можете увидеть, как применять эту модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from models import DiceAligner\n",
    "\n",
    "baseline = DiceAligner(len(t_idx_src), len(t_idx_tgt), threshold=0.01)\n",
    "baseline.fit(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Чтобы оценить качество модели выравнивания, пользуясь имеющейся разметкой, существует ряд автоматических метрик. Они подразумевают, что в разметке есть два вида выравниваний — «уверенные» (sure) и «возможные» (possible). Обозначим для конкретного предложения первое множество выравниваний $S$, второе — $P$, а предсказанные выравнивания — $A$; причём, в отличие от разметки в файле, $S\\subseteq P$. Тогда можно предложить три метрики, используя только операции над этими множествами:\n",
    "\n",
    "Precision $=\\frac{|A\\cap P|}{|A|}$. Отражает, какая доля предсказанных нами выравниваний вообще корректна; если мы дадим в качестве ответа все возможные пары слов в предложении, эта метрика сильно просядет.\n",
    "\n",
    "Recall $=\\frac{|A\\cap S|}{|S|}$. Эта метрика показывает, какую долю «уверенных» выравниваний мы обнаружили. Если мы попытаемся сделать слишком консервативную модель, которая выдаёт 0 или 1 предсказание на нетривиальных предложениях, полнота получится крайне низкая. \n",
    "\n",
    "Alignment Error Rate (AER) $=1-\\frac{|A\\cap P|+|A\\cap S|}{|A|+|S|}$. Метрика является комбинацией двух предыдущих и отслеживает общее качество работы системы, штрафуя оба описанных выше вида нежелаемого поведения модели. \n",
    "\n",
    "**Задание 0. (0.5 балла)** Реализуйте функции compute_precision, compute_recall, compute_aer из модуля metrics.py. Оцените качество бейзлайнового метода. Обратите внимание, что нужно использовать микро-усреднение во всех функциях: необходимо просуммировать числитель и знаменатель по всем предложениям и только потом делить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from metrics import compute_aer\n",
    "\n",
    "compute_aer(all_targets,baseline.align(tokenized_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Теперь мы можем перейти к базовой вероятностной модели для выравнивания слов. Пусть $S=(s_1,\\ldots,s_n)$ исходное предложение, $T=(t_1,\\ldots,t_m)$ — его перевод. В роли латентных переменных будут выступать выравнивания $A=(a_1,\\ldots,a_m)$ каждого слова в целевом предложении, причём $a_i\\in\\{1,\\ldots,n\\}$ (считаем, что каждое слово в $t$ является переводом какого-то слова из $s$). Параметрами модели является матрица условных вероятностей перевода: каждый её элемент $\\theta(y|x)=p(y|x)$ отражает вероятность того, что переводом слова $x$ с исходного языка на целевой является слово $y$ (нормировка, соответственно, совершается по словарю целевого языка). Правдоподобие латентных переменных и предложения на целевом языке в этой модели записывается так:\n",
    "\n",
    "$$\n",
    "p(A,T|S)=\\prod_{i=1}^m p(a_i)p(t_i|a_i,S)=\\prod_{i=1}^m \\frac{1}{n}\\theta(t_i|s_{a_i}).\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Задание 1. (2 балла)** Выведите шаги EM-алгоритма для этой модели, а также получите выражение для подсчёта нижней оценки правдоподобия ($\\mathcal{L}$ в обозначениях лекции и семинара). **Обратите внимание, что на M-шаге нужно найти аналитический максимум по параметрам.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "(∩｀-´)⊃━☆ﾟ.*･｡ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Задание 2. (2.5 балла)** Реализуйте все методы класса `WordAligner` в соответствии с полученными вами формулами. Протестируйте вашу реализацию через Яндекс.Контест, а здесь обучите модель и посчитайте её AER на истинной разметке. Чтобы предсказать выравнивание для пары предложений в этой модели, следует выбирать в соответствие для слова в целевом предложении с индексом $i$ позицию, соответствующую максимуму апостериорного распределения $p(a_i|T,S)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from models import WordAligner\n",
    "\n",
    "word_aligner = WordAligner(len(t_idx_src), len(t_idx_tgt), 20)\n",
    "word_aligner.fit(tokenized_sentences);\n",
    "\n",
    "# ༼つ ಠ益ಠ༽つ ─=≡ΣO))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Заметим, что таблицу вероятностей перевода можно использовать и саму по себе для построения словарей. Пример работы показан ниже: метод хоть и работает, но мягко говоря, неидально — слишком мало данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "idx_token_tgt = {index:token for token, index in t_idx_tgt.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "[idx_token_tgt[i] for i in word_aligner.translation_probs[t_idx_src['Mr']].argsort()[-3:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "[idx_token_tgt[i] for i in word_aligner.translation_probs[t_idx_src['Mrs']].argsort()[-3:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[idx_token_tgt[i] for i in word_aligner.translation_probs[t_idx_src['water']].argsort()[-3:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "[idx_token_tgt[i] for i in word_aligner.translation_probs[t_idx_src['depended']].argsort()[-3:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "[idx_token_tgt[i] for i in word_aligner.translation_probs[t_idx_src['on']].argsort()[-3:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Задание 3. (0.5 балла)** Мы смогли получить матрицу условных вероятностей перевода исходного языка в целевой. Можно ли, пользуясь этой матрицей и ещё какими-то статистиками по параллельному корпусу, получить вероятности перевода целевого языка в исходный? Реализуйте такой метод и приведите ниже пример его работы, показав пару удачных переводов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# (>ω<)ノ—==ΞΞ☆*✲ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Задание 4. (0.5 балла)** Визуализируйте полученные выравнивания для нескольких предложений в виде heatmap: по одной из осей располагаются токены исходного текста, по другой — токены его перевода, на пересечении позиций $i$ и $j$ — 0 либо 1 в зависимости от того, является ли в обученной модели $a_i$ равным $j$. Можете ли вы их проинтерпретировать? Постройте аналогичный график, но без дискретизации, а визуализируя напрямую апостериорное распределение. Можете ли вы найти ситуации, в которых модель не уверена, переводом какого слова является слово $i$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# (•̀ 3 •́)━★☆.*･｡ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Заметим, что при задании модели мы сделали довольно сильное предположение о том, что вероятности выбора слова для выравнивания никак не зависят от позиции слова в целевом предложении. Можно сделать эти вероятности настраиваемыми параметрами, получив прямоугольную матрицу $\\phi_{m,n}(j|i)=p(a_i=j|m,n)$ для каждой пары длин предложений $m,n$: по-прежнему мы получаем распределение над индексами в исходном предложении. Тогда модель приобретает вид\n",
    "$$\n",
    "p(A,T|S)=\\prod_{i=1}^m p(a_i|m,n)p(t_i| a_i, S)=\\prod_{i=1}^m \\phi_{m,n}(a_i|i)\\theta(t_i|s_{a_i}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Задание 5. (1.5 балла)** Выведите шаги EM-алгоритма для этой модели, а также получите выражение для подсчёта нижней оценки правдоподобия."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "ଘ(๑˃̵ᴗ˂̵)━☆ﾟ.*･｡ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Задание 6. (2 балла)** Реализуйте все методы класса `WordPositionAligner`, протестируйте их корректность через Яндекс.Контест. Обучите модель, оцените её качество на истинной разметке и сравните его с качеством предыдущей более простой модели. Проиллюстрируйте влияние стартовых параметров на результат, проинициализировав эту модель параметрами модели из задания 2 (важно, чтобы суммарное число эпох обучения в обоих сценариях оставалось тем же)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from models import WordPositionAligner\n",
    "# (≧ ◡ ≦)━★☆.*･｡ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Задание 7. (1 балл)** В предыдущих пунктах мы никак не заостряли внимание на предобработке текстов, что может негативно влиять на результаты обученной модели. Например, сейчас метод выравнивания учитывает регистр, а слова на чешском языке вдобавок обладают богатой морфологией и большим количеством диакритических знаков. Если сократить количество параметров модели (различных слов), можно ускорить обучение и добиться лучших результатов, потому что статистики по словам будут считаться по большему числу параллельных предложений.\n",
    "\n",
    "Примените к исходным данным [Unicode-нормализацию](https://en.wikipedia.org/wiki/Unicode_equivalence#Normalization), приведите их к нижнему регистру и обучите модель выравнивания заново. Сравните качество и скорость обучения с предыдущими результатами и сделайте выводы. Если вы найдете в данных ещё какие-то проблемы, которые можно исправить более грамотной предобработкой, также продемонстрируйте, как их решение влияет на качество.\n",
    "\n",
    "**Важно:** здесь и далее в процессе обработки данных у вас может получаться, что из тестовых данных будут удалены предложения из-за отсутствия слов в словаре. Если такое всё же произошло, для корректности сравнения считайте AER вашей модели на удалённых предложениях равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# (੭•̀ω•́)੭̸*✩⁺˚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Задание 7. (бонус, до 3 баллов)** \n",
    "\n",
    "Улучшите качество получившейся системы настолько, насколько сможете. За каждые 5 процентов, на которые AER на тех же данных получается меньше, чем минимум ошибки всех предыдущих моделей, вы получите по 1 бонусному баллу.\n",
    "\n",
    "Ниже приведены несколько идей, которые могут помочь вам повысить \n",
    "\n",
    "* Модифицировать модель: как вы можете понять, недостатком второго реализованного вами подхода является избыточное число параметров из-за необходимости подерживать отдельную матрицу для каждой различной пары длин предложений в корпусе. В статье https://www.aclweb.org/anthology/N13-1073.pdf приведён способ снижения числа параметров, задающих априорное распределение позиций выравнивания, который позволяет в десять раз быстрее обучать модель и получать лучшее качество.\n",
    "* Агрегация по двум направлениям: в статье https://www.aclweb.org/anthology/J03-1002/ утверждается, что асимметричность выравниваний вредит качеству, потому что из-за выбранной модели одному слову в целевом предложении не может соответствовать два слова в исходном предложении. Для решения этой проблемы (и улучшения метрик, разумеется) авторы предлагают несколько алгоритмов, которые можно попробовать применить в этом задании.\n",
    "* Использовать больше обучающих данных. В корпусе, которым мы пользуемся, только пара тысяч предложений, чего может не хватать для по-настоящему хорошей модели выравнивания. Разумеется, неразмеченных параллельных английско-чешских корпусов гораздо больше, поэтому можно воспользоваться ими. Хорошая точка для старта — данные с соревнования по машинному переводу  [воркшопа WMT](http://www.statmt.org/wmt20/translation-task.html).\n",
    "* В языках часто существуют слова наподобие артиклей или предлогов, которым не соответствует ни одно слово в переводе. Все рассмотренные в рамках задания модели это не учитывают, возможно, добавление возможности перевода в «нулевой» токен улучшит качество модели (при тестировании такие выравнивания имеет смысл выбрасывать)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ┐_(ツ)_┌━☆ﾟ.*･｡ﾟ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
